>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 7      |
|-----------------|
| Mariem                |   
| Martin              |   
| Miri               |   
| Riya                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

This assignment explores three different techniques for bug testing in software development; exploratory, manual scripted, and regression testing, and familiarizes us with defect tracking systems. Exploratory testing, also called manual non-scripted testing, encourages testers to freely explore the system under test (SUT) as if they were real users of the program. Thus, testers can use any method to uncover bugs or problems in user experience. Because it is not predefined, this method can uncover defects that scripted tests may miss. On the other hand, manual scripted testing uses a predefined testing suite that defines a list of test cases that must be checked. Although the documentation used for this testing allows for ease in repeatability, the predefined nature of the test cases may restrict the number of bugs found. The last bug testing method; regression testing, is used after a system version update. It analyses the SUT again to ensure that the update did not break any of the previously tested test cases. All three of these testing methods are vital to the software development process to ensure that software releases are stable and as defect-free as possible.

# High-level description of the exploratory testing plan

- _Scope_: core functionalities such as account validation and withdrawals
- _Objective_: find critical bugs and ensure system requirements are met
- _Strategy_: breadth-first approach to quickly identify any errors from common paths for all functions. This is to ensure that the system at least delivers the basic requirements for an ATM. A second round of tests will occur afterwards to catch exceptional paths.
- _Reporting_: use bug reporting tool; DevOps to keeps records of bugs found in testing

# Comparison of exploratory and manual functional testing

**Exploratory:** This is an approach to testing that is flexible but has a structured approach where the tester creates an overall high level test plan that specifies what to test and to what expect. For example, you can specify what functions/features to target and whether to test the most common paths or exceptional paths. Due to the tests not being scripted, results from this type of test will vary from different testers. 
- _Effectiveness_: This programming approach is very effective for finding new/unexpected bugs that are missed in the tests.
- _Consistency_: This approach to testing has low consistency compared to other methods as the results vary depending on how the tester approaches it.
- _Efficiency_: This approach to testing can be really efficient at the start as one can find defects quickly from just using the application but after sometime, it can become inefficient as not all tests will be covered.  
- _Trade-offs_: It can be difficult to consistently test the system multiple times in an exploratory testing approach as the tests are not documented in detail ahead of time. It also requires the tester to have a really good understanding of how the system should work to be able to pick up on bugs.
- _Best Use Case_: This method would be better to use during the earlier stages of testing, quickly testing new features or to just see if there is unexpected behavior.

**Manual:** This is an approach to testing that is structured following predefined test cases to verify whether a system behaves how it is expected to or not. This method prioritizes consistency and thorough coverage compared to the exploratory testing approach. 
- _Effectiveness_: This approach to testing is very efficient when it comes to the validation of functionality in order to ensure the exact expected behavior is executed. 
- _Consistency_: This method of testing is highly consistent due to the test cases being predefined, making it a reliable method to be conducted by multiple testers and repeated multiple times.
- _Efficiency_: This method is highly efficient because everything is predefined in detail but less unexpected defects can be found as the scope of the tests is already set in stone.
- _Trade-offs_: Less flexible testing approach.
- _Best Use Case_: This is very good for regression testing, where the tester can check if the new changes affected any of the pre existing functionalities.

# Notes and discussion of the peer reviews of defect reports

- Both pairs in our group read over the defect reports, making sure to check for any inconsistencies with the guidelines. Ensured all the bugs were clearly outlined with the expected vs found output.
- Our discussions led us to add more details to our report such as clear  documentation, fix small mistakes and inconsistencies. As a result, we enhanced the level of detail in our reports, making them more precise and easier to understand.

# How the pair testing was managed and team work/effort was divided 

Each pair worked on each of the testing phases; exploratory, manual and regression testing. Thus, each pair spent similar amounts of time on each test, then came together to discuss our findings after each phase. For each phase, we always swapped who was the one testing, and the one recording the results. We initially used a google doc to record our findings before figuring out how to manage the bug reports on Jira. In the manual scripted testing phase, we split up the work for the test cases so that the first pair works on cases 1-20, and the second pair handles 21-40. Later, we went back to verify these test cases together. The pairs were split up as such: Mariem/Riya, Martin/Miri.

# Difficulties encountered, challenges overcome, and lessons learned

**Difficulties**
- Initially when we started with doing exploratory testing, it was a bit difficult to know what it was that was not working because in many cases, a few steps had already been taken before the defects appeared.

**Challenges**
- Since exploratory testing is unscripted, it was challenging  to find all the bugs, as certain bugs only occurred after interacting with the system multiple times in a specific way. 
Running the manual testing required a lot of attention to details as sometimes we weren't sure which test was causing the bug to show. 
    - For example: there were some inconsistencies when it came to withdrawing money and we were not sure if it was because of the atm machine or the user bank account. 
- We also had a challenging time utilizing JIRA as it was our group’s first time utilizing this tool! But with a lot of research and perseverance, we were able to make it work.

**Lessons Learned**
- Working in pairs and then coming together as a group allowed us to discuss unclear tests from different perspectives to help come to a solution. Additionally, it widened the limited scope that exists with individual exploratory testing because more ideas are generated on how to test each feature.


# Comments/feedback on the lab and lab document itself

- This lab helped us understand how testing works, as it provided us with practical exposure to different types of testing. 
- The suggested working method of doing the tests in pairs was really helpful, as it gave a different insight to all the tests and bugs. 
- More details on how to use, how to format bug reports, and when to use the tool, directly on the lab document would have been helpful when starting this lab as there was confusion on how and when to get started using the tool during the testing process.

